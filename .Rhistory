# ~ Create a list of "lazy" futures that returns one or more asynchronous processes that, when called, starts remote parallel workers
schedule.workers = new.env()
purrr::iwalk(sys.actions, ~{
rlang::inject(future::futureAssign(
x = paste0("worker_", !!.y)
, value = {
# The following call to `Sys.sleep()` gives each worker process time to initialize.
#	The sleep timeout is incremental (".delay" holds the current value of ".y" from the outer call to `imap()`)
#	Without the delay, it's possible to have a process collision where not all workers start, leaving the call to
#	`parallelly::makeClusterPSOCK()` as a hung process waiting for workers to start
# Normally, SSH constructs would be used for multi-node clusters; however, that is not always possible, especially in controlled Windows environments.
# To address this, "psexec64.exe" will be used as this is designed to remotely start processes work in a Windows environment
# Note: 'psexec64' must be globally available (e.g., path set in environment variable)
callr::r_bg(func = function(.args, .delay){
message("starting worker ...");
Sys.sleep(.delay)
system2(command = 'psexec64', args = .args, wait = FALSE, minimized = TRUE, invisible = TRUE)
}, args = list(.args = !!.x, .delay = !!.y))
}
, assign.env = schedule.workers
, lazy = TRUE
, seed = TRUE
))
})
# ~ Sequentially start initializing the workers by resolving the promises
# At this point, a number of separate `callr` processes have been spawned that will successively start the remote workers.
#		This essentially mimics someone on the remote hosts starting a worker process when asked.
# ~ Make the call to 'makeClusterPSOCK':
later::later(function(){ schedule.workers %$% mget(ls()) }, delay = 2);
callr::r_bg(function(i){
do.call(parallelly::makeClusterPSOCK, args = i)
}, args = { rlang::list2(
workers 		= worker.hosts
, master			= master.host
, port				= cluster.port
, manual			= TRUE # `TRUE` in order to use non-SSH methods to remotely start the workers (e.g., psexec64.exe)
, homogeneous	= TRUE # `TRUE` since all cluster nodes (SHOULD) have identical setups
, ...
)
})
}
# ::::: EXTERNAL DATA MANAGEMENT
do.make_cluster <- function(
master.host = sprintf("%s", Sys.getenv("COMPUTERNAME"))
, worker.hosts = "localhost"
, cluster.port = parallelly::freePort(3e4:4.4e4)
, win_cmd = "\\\\%s \"%s\" --default-packages=datasets,utils,grDevices,graphics,stats -e \".libPaths(paste0(R.home(), '/library')); tryCatch(parallel:::.workRSOCK,error=function(e)parallel:::.slaveRSOCK)()\" MASTER=%s PORT=%s OUT=\"/dev/null\" SETUPTIMEOUT=3 TIMEOUT=2592000 XDR=TRUE SETUPSTRATEGY=sequential "
, ...
){
#' A Wrapper for \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @description
#' All arguments are ultimately passed to \code{\link[parallelly]{makeClusterPSOCK}}.  Hard-coded arguments are as follows:
#' \itemize{
#' \item \code{manual}: \code{TRUE}
#' \item \code{homogenous}: \code{TRUE}
#' \item \code{rshcmd}: Replaced with \href{https://docs.microsoft.com/en-us/sysinternals/downloads/pstools}{psexec}, which is why the workers must be started manually in a Windows environment.  Putting the executable in \code{paste0(R.home(), "/bin/x64/Rscript")} ensures the same user access as \code{rscript.exe}
#' \item \code{rscript}: Dynamically set with \code{paste0(R.home(), "/bin/x64/Rscript")}
#' }
#'
#' @details
#' Argument \code{worker.hosts} can take two forms: either a character vector, or a <space/comma/semi-colon>-delimited string.  Additionally, each worker name/IP can have the number of instances to run by appending a colon (':') and providing a positive integer (e.g., \code{"WORKER_1; WORKER_2:3, WORKER_3:9"} resulting in 13 worker processes across three (3) nodes).
#'
#' The function leverages the \code{parallelly} and \code{callr} packages in order to start the workers in the background after the call to \code{\link[parallelly]{makeClusterPSOCK}} is made.  What \emph{actually} occurs is the workers are initiated first via \code{list(future({callr::r_bg(...)}, lazy = TRUE)[i])}, where \code{i} ranges from 1 to the number of parsed workers.  The list of lazy futures is iterated over via \code{lapply(l, future::value)}, spawning background processes that are initially delayed with incrementally increasing timeouts in calls to \code{Sys.sleep(<delay>)}.
#'
#' The result is that the iteration phase is nearly instantaneous; however, because these are delayed in background processes, the subsequent call to \code{\link[parallelly]{makeClusterPSOCK}} is executed in the meantime: while waiting for workers to start, the workers are started once the delay timeouts expire sequentially (like dominoes falling one after another).
#'
#' Calling \code{\link[future]{plan}}(sequential) at the end of the parallel processing block is advised.
#'
#' @param master.host (string) Name or IP of the master node as known to the worker nodes
#' @param worker.hosts (string, character vector) The names or IPs of the worker nodes (see 'Details')
#' @param cluster.port (integer) Not used (retained for legacy code);
#' @param win_cmd (string) The worker initialization command template to use given as an \code{\link[base]{sprintf}} string accepting \emph{ordered} arguments of \emph{worker hosts}, \emph{path to rscript.exe (64-bit) }, \emph{'Master' hostname}, and \emph{port}
#'
#' @param ... Additional arguments passed on to \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @return A socket connection object as per \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @family Parallelism
#'
#' @export
gc(full = TRUE);
force(cluster.port)
# ~ Parse the argument "worker.hosts": only meaningful when it is in the form "HOST:cores HOST2:cores2 ..."
worker.hosts %<>% {
wh = stringi::stri_split_regex(., "[,; ]", omit_empty = TRUE) %>% unlist()
hnms = c(
wh[!wh %like% ":"]
, wh[wh %like% ":"] %>% stringi::stri_split_fixed(":", omit_empty = TRUE) %>%
sapply(function(i){ t(rep(i[1], as.integer(i[2]))) }) %>%
unlist() %>% as.vector()
)
# Limit the number of participating hosts to 20 (max user connections, I think)
if (length(hnms) > 20){ hnms[1:20] } else { hnms }
}
# ~ `sys.actions` holds the Windows command-line strings to start the cluster workers.
sys.actions = sprintf(
win_cmd
, worker.hosts
, paste0(R.home(), "/bin/x64/Rscript")
, master.host
, cluster.port
);
# ~ Create a list of "lazy" futures that returns one or more asynchronous processes that, when called, starts remote parallel workers
schedule.workers = new.env()
purrr::iwalk(sys.actions, ~{
rlang::inject(future::futureAssign(
x = paste0("worker_", !!.y)
, value = {
# The following call to `Sys.sleep()` gives each worker process time to initialize.
#	The sleep timeout is incremental (".delay" holds the current value of ".y" from the outer call to `imap()`)
#	Without the delay, it's possible to have a process collision where not all workers start, leaving the call to
#	`parallelly::makeClusterPSOCK()` as a hung process waiting for workers to start
# Normally, SSH constructs would be used for multi-node clusters; however, that is not always possible, especially in controlled Windows environments.
# To address this, "psexec64.exe" will be used as this is designed to remotely start processes work in a Windows environment
# Note: 'psexec64' must be globally available (e.g., path set in environment variable)
callr::r_bg(func = function(.args, .delay){
message("starting worker ...");
Sys.sleep(.delay)
system2(command = 'psexec64', args = .args, wait = FALSE, minimized = TRUE, invisible = TRUE)
}, args = list(.args = !!.x, .delay = !!.y))
}
, assign.env = schedule.workers
, lazy = TRUE
, seed = TRUE
))
})
# ~ Sequentially start initializing the workers by resolving the promises
# At this point, a number of separate `callr` processes have been spawned that will successively start the remote workers.
#		This essentially mimics someone on the remote hosts starting a worker process when asked.
# ~ Make the call to 'makeClusterPSOCK':
later::later(function(){ schedule.workers %$% mget(ls()) }, delay = 2);
callr::r_bg(function(i){
do.call(parallelly::makeClusterPSOCK, args = i)
}, args = { rlang::list2(
workers 		= worker.hosts
, master			= master.host
, port				= cluster.port
, manual			= TRUE # `TRUE` in order to use non-SSH methods to remotely start the workers (e.g., psexec64.exe)
, homogeneous	= TRUE # `TRUE` since all cluster nodes (SHOULD) have identical setups
, ...
)
})
}
# ~ do.make_cluster(), get.cluster_ports() ====
# debug(do.make_cluster)
library(magrittr);
library(data.table);
assign("test.cluster", do.make_cluster(worker.hosts = "IMPERIALTOWER:5", autoStop = TRUE))
# ::::: EXTERNAL DATA MANAGEMENT
do.make_cluster <- function(
master.host = sprintf("%s", Sys.getenv("COMPUTERNAME"))
, worker.hosts = "localhost"
, cluster.port = parallelly::freePort(3e4:4.4e4)
, win_cmd = "\\\\%s \"%s\" --default-packages=datasets,utils,grDevices,graphics,stats -e \".libPaths(paste0(R.home(), '/library')); tryCatch(parallel:::.workRSOCK,error=function(e)parallel:::.slaveRSOCK)()\" MASTER=%s PORT=%s OUT=\"/dev/null\" SETUPTIMEOUT=3 TIMEOUT=2592000 XDR=TRUE SETUPSTRATEGY=sequential "
, ...
){
#' A Wrapper for \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @description
#' All arguments are ultimately passed to \code{\link[parallelly]{makeClusterPSOCK}}.  Hard-coded arguments are as follows:
#' \itemize{
#' \item \code{manual}: \code{TRUE}
#' \item \code{homogenous}: \code{TRUE}
#' \item \code{rshcmd}: Replaced with \href{https://docs.microsoft.com/en-us/sysinternals/downloads/pstools}{psexec}, which is why the workers must be started manually in a Windows environment.  Putting the executable in \code{paste0(R.home(), "/bin/x64/Rscript")} ensures the same user access as \code{rscript.exe}
#' \item \code{rscript}: Dynamically set with \code{paste0(R.home(), "/bin/x64/Rscript")}
#' }
#'
#' @details
#' Argument \code{worker.hosts} can take two forms: either a character vector, or a <space/comma/semi-colon>-delimited string.  Additionally, each worker name/IP can have the number of instances to run by appending a colon (':') and providing a positive integer (e.g., \code{"WORKER_1; WORKER_2:3, WORKER_3:9"} resulting in 13 worker processes across three (3) nodes).
#'
#' The function leverages the \code{parallelly} and \code{callr} packages in order to start the workers in the background after the call to \code{\link[parallelly]{makeClusterPSOCK}} is made.  What \emph{actually} occurs is the workers are initiated first via \code{list(future({callr::r_bg(...)}, lazy = TRUE)[i])}, where \code{i} ranges from 1 to the number of parsed workers.  The list of lazy futures is iterated over via \code{lapply(l, future::value)}, spawning background processes that are initially delayed with incrementally increasing timeouts in calls to \code{Sys.sleep(<delay>)}.
#'
#' The result is that the iteration phase is nearly instantaneous; however, because these are delayed in background processes, the subsequent call to \code{\link[parallelly]{makeClusterPSOCK}} is executed in the meantime: while waiting for workers to start, the workers are started once the delay timeouts expire sequentially (like dominoes falling one after another).
#'
#' Calling \code{\link[future]{plan}}(sequential) at the end of the parallel processing block is advised.
#'
#' @param master.host (string) Name or IP of the master node as known to the worker nodes
#' @param worker.hosts (string, character vector) The names or IPs of the worker nodes (see 'Details')
#' @param cluster.port (integer) Not used (retained for legacy code);
#' @param win_cmd (string) The worker initialization command template to use given as an \code{\link[base]{sprintf}} string accepting \emph{ordered} arguments of \emph{worker hosts}, \emph{path to rscript.exe (64-bit) }, \emph{'Master' hostname}, and \emph{port}
#'
#' @param ... Additional arguments passed on to \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @return A socket connection object as per \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @family Parallelism
#'
#' @export
gc(full = TRUE);
force(cluster.port)
# ~ Parse the argument "worker.hosts": only meaningful when it is in the form "HOST:cores HOST2:cores2 ..."
worker.hosts %<>% {
wh = stringi::stri_split_regex(., "[,; ]", omit_empty = TRUE) %>% unlist()
hnms = c(
wh[!wh %like% ":"]
, wh[wh %like% ":"] %>% stringi::stri_split_fixed(":", omit_empty = TRUE) %>%
sapply(function(i){ t(rep(i[1], as.integer(i[2]))) }) %>%
unlist() %>% as.vector()
)
# Limit the number of participating hosts to 20 (max user connections, I think)
if (length(hnms) > 20){ hnms[1:20] } else { hnms }
}
# ~ `sys.actions` holds the Windows command-line strings to start the cluster workers.
sys.actions = sprintf(
win_cmd
, worker.hosts
, paste0(R.home(), "/bin/x64/Rscript")
, master.host
, cluster.port
);
# ~ Create a list of "lazy" futures that returns one or more asynchronous processes that, when called, starts remote parallel workers
schedule.workers = new.env()
purrr::iwalk(sys.actions, ~{
rlang::inject(future::futureAssign(
x = paste0("worker_", !!.y)
, value = {
# The following call to `Sys.sleep()` gives each worker process time to initialize.
#	The sleep timeout is incremental (".delay" holds the current value of ".y" from the outer call to `imap()`)
#	Without the delay, it's possible to have a process collision where not all workers start, leaving the call to
#	`parallelly::makeClusterPSOCK()` as a hung process waiting for workers to start
# Normally, SSH constructs would be used for multi-node clusters; however, that is not always possible, especially in controlled Windows environments.
# To address this, "psexec64.exe" will be used as this is designed to remotely start processes work in a Windows environment
# Note: 'psexec64' must be globally available (e.g., path set in environment variable)
callr::r_bg(func = function(.args, .delay){
message("starting worker ...");
Sys.sleep(.delay)
system2(command = 'psexec64', args = .args, wait = FALSE, minimized = TRUE, invisible = TRUE)
}, args = list(.args = !!.x, .delay = !!.y))
}
, assign.env = schedule.workers
, lazy = TRUE
, seed = TRUE
))
})
# ~ Sequentially start initializing the workers by resolving the promises
# At this point, a number of separate `callr` processes have been spawned that will successively start the remote workers.
#		This essentially mimics someone on the remote hosts starting a worker process when asked.
# ~ Make the call to 'makeClusterPSOCK':
future::values(schedule.workers)
callr::r_bg(function(i){
do.call(parallelly::makeClusterPSOCK, args = i)
}, args = { rlang::list2(
workers 		= worker.hosts
, master			= master.host
, port				= cluster.port
, manual			= TRUE # `TRUE` in order to use non-SSH methods to remotely start the workers (e.g., psexec64.exe)
, homogeneous	= TRUE # `TRUE` since all cluster nodes (SHOULD) have identical setups
, ...
)
})
}
# ~ do.make_cluster(), get.cluster_ports() ====
# debug(do.make_cluster)
library(magrittr);
library(data.table);
assign("test.cluster", do.make_cluster(worker.hosts = "IMPERIALTOWER:5", autoStop = TRUE))
# ::::: EXTERNAL DATA MANAGEMENT
do.make_cluster <- function(
master.host = sprintf("%s", Sys.getenv("COMPUTERNAME"))
, worker.hosts = "localhost"
, cluster.port = parallelly::freePort(3e4:4.4e4)
, win_cmd = "\\\\%s \"%s\" --default-packages=datasets,utils,grDevices,graphics,stats -e \".libPaths(paste0(R.home(), '/library')); tryCatch(parallel:::.workRSOCK,error=function(e)parallel:::.slaveRSOCK)()\" MASTER=%s PORT=%s OUT=\"/dev/null\" SETUPTIMEOUT=3 TIMEOUT=2592000 XDR=TRUE SETUPSTRATEGY=sequential "
, ...
){
#' A Wrapper for \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @description
#' All arguments are ultimately passed to \code{\link[parallelly]{makeClusterPSOCK}}.  Hard-coded arguments are as follows:
#' \itemize{
#' \item \code{manual}: \code{TRUE}
#' \item \code{homogenous}: \code{TRUE}
#' \item \code{rshcmd}: Replaced with \href{https://docs.microsoft.com/en-us/sysinternals/downloads/pstools}{psexec}, which is why the workers must be started manually in a Windows environment.  Putting the executable in \code{paste0(R.home(), "/bin/x64/Rscript")} ensures the same user access as \code{rscript.exe}
#' \item \code{rscript}: Dynamically set with \code{paste0(R.home(), "/bin/x64/Rscript")}
#' }
#'
#' @details
#' Argument \code{worker.hosts} can take two forms: either a character vector, or a <space/comma/semi-colon>-delimited string.  Additionally, each worker name/IP can have the number of instances to run by appending a colon (':') and providing a positive integer (e.g., \code{"WORKER_1; WORKER_2:3, WORKER_3:9"} resulting in 13 worker processes across three (3) nodes).
#'
#' The function leverages the \code{parallelly} and \code{callr} packages in order to start the workers in the background after the call to \code{\link[parallelly]{makeClusterPSOCK}} is made.  What \emph{actually} occurs is the workers are initiated first via \code{list(future({callr::r_bg(...)}, lazy = TRUE)[i])}, where \code{i} ranges from 1 to the number of parsed workers.  The list of lazy futures is iterated over via \code{lapply(l, future::value)}, spawning background processes that are initially delayed with incrementally increasing timeouts in calls to \code{Sys.sleep(<delay>)}.
#'
#' The result is that the iteration phase is nearly instantaneous; however, because these are delayed in background processes, the subsequent call to \code{\link[parallelly]{makeClusterPSOCK}} is executed in the meantime: while waiting for workers to start, the workers are started once the delay timeouts expire sequentially (like dominoes falling one after another).
#'
#' Calling \code{\link[future]{plan}}(sequential) at the end of the parallel processing block is advised.
#'
#' @param master.host (string) Name or IP of the master node as known to the worker nodes
#' @param worker.hosts (string, character vector) The names or IPs of the worker nodes (see 'Details')
#' @param cluster.port (integer) Not used (retained for legacy code);
#' @param win_cmd (string) The worker initialization command template to use given as an \code{\link[base]{sprintf}} string accepting \emph{ordered} arguments of \emph{worker hosts}, \emph{path to rscript.exe (64-bit) }, \emph{'Master' hostname}, and \emph{port}
#'
#' @param ... Additional arguments passed on to \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @return A socket connection object as per \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @family Parallelism
#'
#' @export
gc(full = TRUE);
force(cluster.port)
# ~ Parse the argument "worker.hosts": only meaningful when it is in the form "HOST:cores HOST2:cores2 ..."
worker.hosts %<>% {
wh = stringi::stri_split_regex(., "[,; ]", omit_empty = TRUE) %>% unlist()
hnms = c(
wh[!wh %like% ":"]
, wh[wh %like% ":"] %>% stringi::stri_split_fixed(":", omit_empty = TRUE) %>%
sapply(function(i){ t(rep(i[1], as.integer(i[2]))) }) %>%
unlist() %>% as.vector()
)
# Limit the number of participating hosts to 20 (max user connections, I think)
if (length(hnms) > 20){ hnms[1:20] } else { hnms }
}
# ~ `sys.actions` holds the Windows command-line strings to start the cluster workers.
sys.actions = sprintf(
win_cmd
, worker.hosts
, paste0(R.home(), "/bin/x64/Rscript")
, master.host
, cluster.port
);
# ~ Create a list of "lazy" futures that returns one or more asynchronous processes that, when called, starts remote parallel workers
schedule.workers = new.env()
purrr::iwalk(sys.actions, ~{
rlang::inject(future::futureAssign(
x = paste0("worker_", !!.y)
, value = {
# The following call to `Sys.sleep()` gives each worker process time to initialize.
#	The sleep timeout is incremental (".delay" holds the current value of ".y" from the outer call to `imap()`)
#	Without the delay, it's possible to have a process collision where not all workers start, leaving the call to
#	`parallelly::makeClusterPSOCK()` as a hung process waiting for workers to start
# Normally, SSH constructs would be used for multi-node clusters; however, that is not always possible, especially in controlled Windows environments.
# To address this, "psexec64.exe" will be used as this is designed to remotely start processes work in a Windows environment
# Note: 'psexec64' must be globally available (e.g., path set in environment variable)
callr::r_bg(func = function(.args, .delay){
message("starting worker ...");
Sys.sleep(.delay)
system2(command = 'psexec64', args = .args, wait = FALSE, minimized = TRUE, invisible = TRUE)
}, args = list(.args = !!.x, .delay = !!.y))
}
, assign.env = schedule.workers
, lazy = TRUE
, seed = TRUE
))
})
# ~ Sequentially start initializing the workers by resolving the promises
# At this point, a number of separate `callr` processes have been spawned that will successively start the remote workers.
#		This essentially mimics someone on the remote hosts starting a worker process when asked.
# ~ Make the call to 'makeClusterPSOCK':
future::value(schedule.workers)
callr::r_bg(function(i){
do.call(parallelly::makeClusterPSOCK, args = i)
}, args = { rlang::list2(
workers 		= worker.hosts
, master			= master.host
, port				= cluster.port
, manual			= TRUE # `TRUE` in order to use non-SSH methods to remotely start the workers (e.g., psexec64.exe)
, homogeneous	= TRUE # `TRUE` since all cluster nodes (SHOULD) have identical setups
, ...
)
})
}
assign("test.cluster", do.make_cluster(worker.hosts = "IMPERIALTOWER:5", autoStop = TRUE))
# undebug(do.make_cluster)
test.cluster$main$is_alive()
View(test.cluster)
# undebug(do.make_cluster)
test.cluster$is_alive()
test.cluster$get_result()
# ::::: EXTERNAL DATA MANAGEMENT
do.make_cluster <- function(
master.host = sprintf("%s", Sys.getenv("COMPUTERNAME"))
, worker.hosts = "localhost"
, cluster.port = parallelly::freePort(3e4:4.4e4)
, win_cmd = "\\\\%s \"%s\" --default-packages=datasets,utils,grDevices,graphics,stats -e \".libPaths(paste0(R.home(), '/library')); tryCatch(parallel:::.workRSOCK,error=function(e)parallel:::.slaveRSOCK)()\" MASTER=%s PORT=%s OUT=\"/dev/null\" SETUPTIMEOUT=3 TIMEOUT=2592000 XDR=TRUE SETUPSTRATEGY=sequential "
, ...
){
#' A Wrapper for \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @description
#' All arguments are ultimately passed to \code{\link[parallelly]{makeClusterPSOCK}}.  Hard-coded arguments are as follows:
#' \itemize{
#' \item \code{manual}: \code{TRUE}
#' \item \code{homogenous}: \code{TRUE}
#' \item \code{rshcmd}: Replaced with \href{https://docs.microsoft.com/en-us/sysinternals/downloads/pstools}{psexec}, which is why the workers must be started manually in a Windows environment.  Putting the executable in \code{paste0(R.home(), "/bin/x64/Rscript")} ensures the same user access as \code{rscript.exe}
#' \item \code{rscript}: Dynamically set with \code{paste0(R.home(), "/bin/x64/Rscript")}
#' }
#'
#' @details
#' Argument \code{worker.hosts} can take two forms: either a character vector, or a <space/comma/semi-colon>-delimited string.  Additionally, each worker name/IP can have the number of instances to run by appending a colon (':') and providing a positive integer (e.g., \code{"WORKER_1; WORKER_2:3, WORKER_3:9"} resulting in 13 worker processes across three (3) nodes).
#'
#' The function leverages the \code{parallelly} and \code{callr} packages in order to start the workers in the background after the call to \code{\link[parallelly]{makeClusterPSOCK}} is made.  What \emph{actually} occurs is the workers are initiated first via \code{list(future({callr::r_bg(...)}, lazy = TRUE)[i])}, where \code{i} ranges from 1 to the number of parsed workers.  The list of lazy futures is iterated over via \code{lapply(l, future::value)}, spawning background processes that are initially delayed with incrementally increasing timeouts in calls to \code{Sys.sleep(<delay>)}.
#'
#' The result is that the iteration phase is nearly instantaneous; however, because these are delayed in background processes, the subsequent call to \code{\link[parallelly]{makeClusterPSOCK}} is executed in the meantime: while waiting for workers to start, the workers are started once the delay timeouts expire sequentially (like dominoes falling one after another).
#'
#' Calling \code{\link[future]{plan}}(sequential) at the end of the parallel processing block is advised.
#'
#' @param master.host (string) Name or IP of the master node as known to the worker nodes
#' @param worker.hosts (string, character vector) The names or IPs of the worker nodes (see 'Details')
#' @param cluster.port (integer) Not used (retained for legacy code);
#' @param win_cmd (string) The worker initialization command template to use given as an \code{\link[base]{sprintf}} string accepting \emph{ordered} arguments of \emph{worker hosts}, \emph{path to rscript.exe (64-bit) }, \emph{'Master' hostname}, and \emph{port}
#'
#' @param ... Additional arguments passed on to \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @return A socket connection object as per \code{\link[parallelly]{makeClusterPSOCK}}
#'
#' @family Parallelism
#'
#' @export
gc(full = TRUE);
force(cluster.port)
# ~ Parse the argument "worker.hosts": only meaningful when it is in the form "HOST:cores HOST2:cores2 ..."
worker.hosts %<>% {
wh = stringi::stri_split_regex(., "[,; ]", omit_empty = TRUE) %>% unlist()
hnms = c(
wh[!wh %like% ":"]
, wh[wh %like% ":"] %>% stringi::stri_split_fixed(":", omit_empty = TRUE) %>%
sapply(function(i){ t(rep(i[1], as.integer(i[2]))) }) %>%
unlist() %>% as.vector()
)
# Limit the number of participating hosts to 20 (max user connections, I think)
if (length(hnms) > 20){ hnms[1:20] } else { hnms }
}
# ~ `sys.actions` holds the Windows command-line strings to start the cluster workers.
sys.actions = sprintf(
win_cmd
, worker.hosts
, paste0(R.home(), "/bin/x64/Rscript")
, master.host
, cluster.port
);
# ~ Create a list of "lazy" futures that returns one or more asynchronous processes that, when called, starts remote parallel workers
schedule.workers = new.env()
purrr::iwalk(sys.actions, ~{
rlang::inject(future::futureAssign(
x = paste0("worker_", !!.y)
, value = {
# The following call to `Sys.sleep()` gives each worker process time to initialize.
#	The sleep timeout is incremental (".delay" holds the current value of ".y" from the outer call to `imap()`)
#	Without the delay, it's possible to have a process collision where not all workers start, leaving the call to
#	`parallelly::makeClusterPSOCK()` as a hung process waiting for workers to start
# Normally, SSH constructs would be used for multi-node clusters; however, that is not always possible, especially in controlled Windows environments.
# To address this, "psexec64.exe" will be used as this is designed to remotely start processes work in a Windows environment
# Note: 'psexec64' must be globally available (e.g., path set in environment variable)
callr::r_bg(func = function(.args, .delay){
message("starting worker ...");
Sys.sleep(.delay)
system2(command = 'psexec64', args = .args, wait = FALSE, minimized = TRUE, invisible = TRUE)
}, args = list(.args = !!.x, .delay = !!.y))
}
, assign.env = schedule.workers
, lazy = TRUE
, seed = TRUE
))
})
# ~ Sequentially start initializing the workers by resolving the promises
# At this point, a number of separate `callr` processes have been spawned that will successively start the remote workers.
#		This essentially mimics someone on the remote hosts starting a worker process when asked.
# ~ Make the call to 'makeClusterPSOCK':
future::value(schedule.workers)
callr::r_bg(function(i){ do.call(parallelly::makeClusterPSOCK, args = i) } , args = list(i = { rlang::list2(
workers 		= worker.hosts
, master			= master.host
, port				= cluster.port
, manual			= TRUE # `TRUE` in order to use non-SSH methods to remotely start the workers (e.g., psexec64.exe)
, homogeneous	= TRUE # `TRUE` since all cluster nodes (SHOULD) have identical setups
, ...
)
}))
}
assign("test.cluster", do.make_cluster(worker.hosts = "IMPERIALTOWER:5", autoStop = TRUE))
# undebug(do.make_cluster)
test.cluster$is_alive()
plan(tweak(cluster, workers = test.cluster$get_result()))
future::plan(future::tweak(cluster, workers = test.cluster$get_result()))
??tweak
??cluster
future::plan(cluster, workers = test.cluster$get_result())
View(test.cluster)
plan(sequential)
future::plan(sequential)
library(future)
future::plan(sequential)
test.cluster$get_result()
assign("test.cluster", do.make_cluster(worker.hosts = "IMPERIALTOWER:5", autoStop = TRUE))
test.cluster$get_result()
plan(sequential)
test.cluster$get_result()
library(book.of.workflow)
library(book.of.workflow)
library(book.of.workflow)
library(book.of.workflow)
library(book.of.workflow)
library(magick)
book_img <- magick::image_read("book-305126_1280.png")
library(jsonlite)
jsonlite::base64_enc(book_img)
book_img
jsonlite::serializeJSON()
jsonlite::serializeJSON(book_img)
library(sodium)
detach("package:jsonlite", unload = TRUE)
serialize(book_img)
serialize(book_img, connection = NULL)
serialize(book_img, connection = NULL) %>% base64enc::base64encode()
book_img <-
magick::image_read("book-305126_1280.png") %>% serialize(book_img, connection = NULL) %>% base64enc::base64encode()
magick::image_read("book-305126_1280.png") %>% serialize(connection = NULL) %>% base64enc::base64encode()
magick::image_read("book-305126_1280.png") %>% serialize(connection = NULL) %>% base64enc::base64encode() %>% writeClipboard()
library(book.of.workflow)
library(book.of.workflow)
library(book.of.workflow)
library(book.of.workflow)
library(book.of.workflow)
